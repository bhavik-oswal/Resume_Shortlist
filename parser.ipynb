{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0s9VIWtXVBw",
    "outputId": "f930b8d4-59a6-44bd-d6f6-132c6c6e5e49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
      "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.5.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install pdfplumber\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "my-JYMd2Xi9y",
    "outputId": "00ff87c6-6695-4ffb-af84-00ce6a60ec4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l9l6WeXOXq4r",
    "outputId": "deb8b14b-dac5-49fc-cf11-39a91c61038d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing entry-level-data-analyst-resume-example.pdf\n",
      "Processing Resume-2 (1) (1).pdf\n",
      "Processing data-analyst-intern-resume-example.pdf\n",
      "Processing junior-data-analyst-resume-example.pdf\n",
      "Processing resume_updated.pdf\n",
      "Processing data-analyst-resume-example.pdf\n",
      "Data saved to /content/resumes/extracted_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import spacy\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load SpaCy for skills extraction\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_name(text):\n",
    "    \"\"\"\n",
    "    This function returns a candidate name from a list of text\n",
    "    :param text: list of text\n",
    "    :return: string of a candidate name\n",
    "    \"\"\"\n",
    "    # Tokenizes whole text to sentences\n",
    "    Sentences = sent_tokenize(text)\n",
    "    t = []\n",
    "\n",
    "    for s in Sentences:\n",
    "        # Tokenizes sentences to words\n",
    "        t.append(word_tokenize(s))\n",
    "    # Tags a word with its part of speech\n",
    "    words = [pos_tag(token) for token in t]\n",
    "    n = []\n",
    "    for x in words:\n",
    "        for l in x:\n",
    "            # Match matches the pos tag of a word to a given tag\n",
    "            if re.match('[NN.*]', l[1]):\n",
    "                n.append(l[0])\n",
    "\n",
    "    cands = []\n",
    "    for nouns in n:\n",
    "        if not wordnet.synsets(nouns):\n",
    "            cands.append(nouns)\n",
    "\n",
    "    cand = ' '.join(cands[:1])\n",
    "    return cand\n",
    "\n",
    "# Function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    extracted_text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                extracted_text += page.extract_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "    return extracted_text\n",
    "\n",
    "# Function to preprocess text (cleaning)\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Function to extract contact info\n",
    "def extract_contact_info(text):\n",
    "    email_regex = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "    phone_regex = r'\\b\\d{10}\\b|\\b(\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b'\n",
    "    github_regex = r'https://github.com/[A-Za-z0-9_-]+'\n",
    "\n",
    "    email = re.search(email_regex, text)\n",
    "    phone = re.search(phone_regex, text)\n",
    "    github = re.search(github_regex, text)\n",
    "\n",
    "    return {\n",
    "        \"email\": email.group() if email else None,\n",
    "        \"phone\": phone.group() if phone else None,\n",
    "        \"github\": github.group() if github else None\n",
    "    }\n",
    "\n",
    "# Function to extract skills using spaCy\n",
    "def extract_skills(text):\n",
    "    skill_keywords =  [\n",
    "    '.net', '1password', '3d', '3d-reconstruction', 'aboutness',\n",
    "    'abstract-data-type', 'abstract-interpretation', 'abstract-machine',\n",
    "    'access-control', 'access-method', 'access-network', 'accounting',\n",
    "    'active-appearance-model', 'active-database', 'active-networking',\n",
    "    'active-shape-model', 'apache-activemq', 'activity-recognition',\n",
    "    'actuarial-science', 'actuator', 'adaboost', 'adaptive-routing',\n",
    "    'adaptive-system', 'adder', 'adobe-illustrator', 'adobe-photoshop',\n",
    "    'adobe-xd', 'advertising', 'aerial-photography', 'aeronautics',\n",
    "    'aerospace-engineering', 'aerospike', 'agile-project-management',\n",
    "    'agricultural-engineering', 'apache-airflow', 'airtable', 'ajax',\n",
    "    'akamai', 'akka', 'algolia', 'algorithm', 'algorithm-design',\n",
    "    'alpine-linux', 'amazon-api-gateway', 'amazon-athena', 'amazon-cloudfront',\n",
    "    'amazon-cloudwatch', 'amazon-cognito', 'amazon-dynamodb', 'amazon-ebs',\n",
    "    'amazon-ec2', 'amazon-eks', 'amazon-elasticache', 'amazon-elasticsearch-service',\n",
    "    'amazon-emr', 'amazon-kinesis', 'amazon-kinesis-firehose', 'amazon-machine-learning',\n",
    "    'amazon-rds', 'amazon-redshift', 'amazon-route-53', 'amazon-s3',\n",
    "    'amazon-ses', 'amazon-sns', 'amazon-sqs', 'amazon-vpc', 'ambiguity',\n",
    "    'am-php', 'amplitude', 'analog-to-digital-converter', 'analysis-of-algorithms',\n",
    "    'analytics', 'android', 'android-sdk', 'android-studio', 'angular',\n",
    "    'ansible', 'ant-design', 'apache-ant', 'apache-cordova', 'apache-flink',\n",
    "    'apache-http-server', 'apache-maven', 'apache-mesos', 'apache-spark',\n",
    "    'apache-tomcat', 'api', 'api-tools', 'apollo', 'appium', 'arangodb',\n",
    "    'arduino', 'artificial-intelligence', 'asana', 'asp.net', 'aws',\n",
    "    'azure', 'babel', 'bash', 'bayesian-inference', 'big-data',\n",
    "    'bootstrap', 'bot', 'c', 'cakephp', 'celery', 'centos', 'circleci',\n",
    "    'clojure', 'cloudflare', 'cloudinary', 'cobol', 'codeigniter',\n",
    "    'coding', 'data-visualization', 'django', 'docker', 'flask', 'git',\n",
    "    'google-cloud', 'html', 'java', 'javascript', 'jenkins', 'kubernetes',\n",
    "    'linux', 'machine-learning', 'mysql', 'node.js', 'numpy', 'php',\n",
    "    'pytorch', 'python', 'react', 'ruby', 'scala', 'scrum', 'sql',\n",
    "    'tensorflow', 'typescript','vue.js']\n",
    "    doc = nlp(text)\n",
    "\n",
    "    extracted_skills = set()\n",
    "    for token in doc:\n",
    "        if token.text in skill_keywords:\n",
    "            extracted_skills.add(token.text)\n",
    "    return list(extracted_skills)\n",
    "\n",
    "# Function to extract total experience in years\n",
    "def extract_experience(text):\n",
    "    experience_patterns = [\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*(?:years?|yrs?)(?:\\s*of)?\\s*(?:total\\s*)?(?:work\\s*)?experience',\n",
    "        r'total\\s*experience[:\\s](\\d+(?:\\.\\d+)?)\\s(?:years?|yrs?)',\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*(?:years?|yrs?)\\s*(?:of\\s*)?(?:professional\\s*)?experience',\n",
    "        r'work\\s*experience[:\\s](\\d+(?:\\.\\d+)?)\\s(?:years?|yrs?)',\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*(?:year|years)\\s*(?:of\\s*)?(?:total\\s*)?experience'\n",
    "    ]\n",
    "\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    for pattern in experience_patterns:\n",
    "        match = re.search(pattern, text_lower)\n",
    "        if match:\n",
    "            try:\n",
    "                return float(match.group(1))\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "\n",
    "    job_experience_pattern = r'(?:from|)\\s*(\\d{4})\\s*(?:to|[-])\\s*(?:present|current|(\\d{4}))'\n",
    "    job_dates = re.findall(job_experience_pattern, text_lower)\n",
    "\n",
    "    if job_dates:\n",
    "        current_year = datetime.datetime.now().year\n",
    "        total_years = 0\n",
    "\n",
    "        for start, end in job_dates:\n",
    "            start_year = int(start)\n",
    "            end_year = int(end) if end else current_year\n",
    "            total_years += end_year - start_year\n",
    "\n",
    "        return total_years\n",
    "\n",
    "    return \"0\"\n",
    "\n",
    "# Function to process all resumes in a folder and extract data\n",
    "def process_resumes_in_folder(folder_path):\n",
    "    resume_data = []\n",
    "    unique_id = 1  # Start the ID from 1\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            print(f\"Processing {filename}\")\n",
    "\n",
    "            raw_text = extract_text_from_pdf(pdf_path)\n",
    "            if raw_text:\n",
    "                cleaned_text = preprocess_text(raw_text)\n",
    "\n",
    "                contact_info = extract_contact_info(cleaned_text)\n",
    "                skills = extract_skills(cleaned_text)\n",
    "                experience_years = extract_experience(cleaned_text)\n",
    "\n",
    "                resume_data.append({\n",
    "                    \"unique_id\": unique_id,\n",
    "                    \"name\": get_name(cleaned_text),\n",
    "                    \"email\": contact_info[\"email\"],\n",
    "                    \"phone\": contact_info[\"phone\"],\n",
    "                    \"github\": contact_info[\"github\"],\n",
    "                    \"experience_years\": experience_years,\n",
    "                    \"skills\": \", \".join(skills)\n",
    "                })\n",
    "\n",
    "                unique_id += 1\n",
    "\n",
    "    return resume_data\n",
    "\n",
    "# Save the extracted data to a CSV\n",
    "def save_to_csv(resume_data, output_file):\n",
    "    df = pd.DataFrame(resume_data)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "\n",
    "# Main script to run the extraction process\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/content/resumes\"  # Set the path to your folder of resumes\n",
    "    output_file = \"/content/resumes/extracted_data.csv\"  # Path to save the CSV output\n",
    "\n",
    "    resume_data = process_resumes_in_folder(folder_path)\n",
    "    save_to_csv(resume_data, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cE11kTr7Xwfy",
    "outputId": "bfb187c7-5fd6-4751-b306-1e4f70a22abe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing entry-level-data-analyst-resume-example.pdf\n",
      "Processing Resume-2 (1) (1).pdf\n",
      "Processing data-analyst-intern-resume-example.pdf\n",
      "Processing junior-data-analyst-resume-example.pdf\n",
      "Processing resume_updated.pdf\n",
      "Processing data-analyst-resume-example.pdf\n",
      "Data saved to /content/resumes/extracted_data_with_promotions.csv\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import spacy\n",
    "import datetime\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Load SpaCy for skills extraction\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    extracted_text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                extracted_text += page.extract_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "    return extracted_text\n",
    "\n",
    "# Function to preprocess text (cleaning)\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Function to extract contact info (email, phone, github)\n",
    "def extract_contact_info(text):\n",
    "    email_regex = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "    phone_regex = r'\\b\\d{10}\\b|\\b(\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b'\n",
    "    github_regex = r'https://github.com/[A-Za-z0-9_-]+'\n",
    "\n",
    "    email = re.search(email_regex, text)\n",
    "    phone = re.search(phone_regex, text)\n",
    "    github = re.search(github_regex, text)\n",
    "\n",
    "    return {\n",
    "        \"email\": email.group() if email else None,\n",
    "        \"phone\": phone.group() if phone else None,\n",
    "        \"github\": github.group() if github else None\n",
    "    }\n",
    "\n",
    "# Function to extract skills using spaCy\n",
    "def extract_skills(text):\n",
    "    skill_keywords = ['Python', 'Machine Learning', 'NLP', 'Data Analysis', 'Java', 'SQL',\n",
    "    'C++', 'AWS', 'Docker', 'TensorFlow', 'Keras', 'React', 'Node.js''.net', '1password', '3d', '3d-reconstruction', 'aboutness',\n",
    "    'abstract-data-type', 'abstract-interpretation', 'abstract-machine',\n",
    "    'access-control', 'access-method', 'access-network', 'accounting',\n",
    "    'active-appearance-model', 'active-database', 'active-networking',\n",
    "    'active-shape-model', 'apache-activemq', 'activity-recognition',\n",
    "    'actuarial-science', 'actuator', 'adaboost', 'adaptive-routing',\n",
    "    'adaptive-system', 'adder', 'adobe-illustrator', 'adobe-photoshop',\n",
    "    'adobe-xd', 'advertising', 'aerial-photography', 'aeronautics',\n",
    "    'aerospace-engineering', 'aerospike', 'agile-project-management',\n",
    "    'agricultural-engineering', 'apache-airflow', 'airtable', 'ajax',\n",
    "    'akamai', 'akka', 'algolia', 'algorithm', 'algorithm-design',\n",
    "    'alpine-linux', 'amazon-api-gateway', 'amazon-athena', 'amazon-cloudfront',\n",
    "    'amazon-cloudwatch', 'amazon-cognito', 'amazon-dynamodb', 'amazon-ebs',\n",
    "    'amazon-ec2', 'amazon-eks', 'amazon-elasticache', 'amazon-elasticsearch-service',\n",
    "    'amazon-emr', 'amazon-kinesis', 'amazon-kinesis-firehose', 'amazon-machine-learning',\n",
    "    'amazon-rds', 'amazon-redshift', 'amazon-route-53', 'amazon-s3',\n",
    "    'amazon-ses', 'amazon-sns', 'amazon-sqs', 'amazon-vpc', 'ambiguity',\n",
    "    'am-php', 'amplitude', 'analog-to-digital-converter', 'analysis-of-algorithms',\n",
    "    'analytics', 'android', 'android-sdk', 'android-studio', 'angular',\n",
    "    'ansible', 'ant-design', 'apache-ant', 'apache-cordova', 'apache-flink',\n",
    "    'apache-http-server', 'apache-maven', 'apache-mesos', 'apache-spark',\n",
    "    'apache-tomcat', 'api', 'api-tools', 'apollo', 'appium', 'arangodb',\n",
    "    'arduino', 'artificial-intelligence', 'asana', 'asp.net', 'aws',\n",
    "    'azure', 'babel', 'bash', 'bayesian-inference', 'big-data',\n",
    "    'bootstrap', 'bot', 'c', 'cakephp', 'celery', 'centos', 'circleci',\n",
    "    'clojure', 'cloudflare', 'cloudinary', 'cobol', 'codeigniter',\n",
    "    'coding', 'data-visualization', 'django', 'docker', 'flask', 'git',\n",
    "    'google-cloud', 'html', 'java', 'javascript', 'jenkins', 'kubernetes',\n",
    "    'linux', 'machine-learning', 'mysql', 'node.js', 'numpy', 'php',\n",
    "    'pytorch', 'python', 'react', 'ruby', 'scala', 'scrum', 'sql',\n",
    "    'tensorflow', 'typescript','vue.js']\n",
    "    doc = nlp(text)\n",
    "\n",
    "    extracted_skills = set()\n",
    "    for token in doc:\n",
    "        if token.text in skill_keywords:\n",
    "            extracted_skills.add(token.text)\n",
    "    return list(extracted_skills)\n",
    "\n",
    "# Function to extract total experience in years\n",
    "def extract_experience(text):\n",
    "    experience_patterns = [\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*(?:years?|yrs?)(?:\\s*of)?\\s*(?:total\\s*)?(?:work\\s*)?experience',\n",
    "        r'total\\s*experience[:\\s](\\d+(?:\\.\\d+)?)\\s(?:years?|yrs?)',\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*(?:years?|yrs?)\\s*(?:of\\s*)?(?:professional\\s*)?experience',\n",
    "        r'work\\s*experience[:\\s](\\d+(?:\\.\\d+)?)\\s(?:years?|yrs?)',\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*(?:year|years)\\s*(?:of\\s*)?(?:total\\s*)?experience'\n",
    "    ]\n",
    "\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    for pattern in experience_patterns:\n",
    "        match = re.search(pattern, text_lower)\n",
    "        if match:\n",
    "            try:\n",
    "                return float(match.group(1))\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "\n",
    "    job_experience_pattern = r'(?:from|)\\s*(\\d{4})\\s*(?:to|[-])\\s*(?:present|current|(\\d{4}))'\n",
    "    job_dates = re.findall(job_experience_pattern, text_lower)\n",
    "\n",
    "    if job_dates:\n",
    "        current_year = datetime.datetime.now().year\n",
    "        total_years = 0\n",
    "\n",
    "        for start, end in job_dates:\n",
    "            start_year = int(start)\n",
    "            end_year = int(end) if end else current_year\n",
    "            total_years += end_year - start_year\n",
    "\n",
    "        return total_years\n",
    "\n",
    "    return \"0\"\n",
    "\n",
    "# Function to extract promotions count from job descriptions\n",
    "def extract_promotions(text):\n",
    "    # Phrases indicating promotion\n",
    "    promotion_keywords = [\n",
    "        r'promoted', r'promotion', r'lead', r'senior', r'head', r'manager', r'director', r'chief'\n",
    "    ]\n",
    "\n",
    "    promotion_count = 0\n",
    "    for keyword in promotion_keywords:\n",
    "        promotion_count += len(re.findall(keyword, text.lower()))\n",
    "\n",
    "    return promotion_count\n",
    "\n",
    "# Function to process all resumes in a folder and extract data\n",
    "def process_resumes_in_folder(folder_path):\n",
    "    resume_data = []\n",
    "    unique_id = 1  # Start the ID from 1\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            print(f\"Processing {filename}\")\n",
    "\n",
    "            raw_text = extract_text_from_pdf(pdf_path)\n",
    "            if raw_text:\n",
    "                cleaned_text = preprocess_text(raw_text)\n",
    "\n",
    "                contact_info = extract_contact_info(cleaned_text)\n",
    "                skills = extract_skills(cleaned_text)\n",
    "                experience_years = extract_experience(cleaned_text)\n",
    "                promotions = extract_promotions(cleaned_text)\n",
    "\n",
    "                resume_data.append({\n",
    "                    \"unique_id\": unique_id,\n",
    "                    \"name\": get_name(cleaned_text),\n",
    "                    \"email\": contact_info[\"email\"],\n",
    "                    \"phone\": contact_info[\"phone\"],\n",
    "                    \"github\": contact_info[\"github\"],\n",
    "                    \"experience_years\": experience_years,\n",
    "                    \"skills\": \", \".join(skills),\n",
    "                    \"promotions\": promotions  # Add promotions count here\n",
    "                })\n",
    "\n",
    "                unique_id += 1\n",
    "\n",
    "    return resume_data\n",
    "\n",
    "# Save the extracted data to a CSV\n",
    "def save_to_csv(resume_data, output_file):\n",
    "    df = pd.DataFrame(resume_data)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "\n",
    "# Main script to run the extraction process\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/content/resumes\"  # Set the path to your folder of resumes\n",
    "    output_file = \"/content/resumes/extracted_data_with_promotions.csv\"  # Path to save the CSV output\n",
    "\n",
    "    resume_data = process_resumes_in_folder(folder_path)\n",
    "    save_to_csv(resume_data, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXK9rHKodg-g"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
