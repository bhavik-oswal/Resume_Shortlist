{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 spacy pdfplumber scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbhJ1uNP1zOf",
        "outputId": "74cc51aa-14d8-4e0f-924d-f9fca3849bea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, PyPDF2, pdfminer.six, pdfplumber\n",
            "Successfully installed PyPDF2-3.0.1 pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "\n",
        "class ResumeParser:\n",
        "    def __init__(self):\n",
        "        # GitHub API URL to fetch public repositories\n",
        "        self.github_api_url = 'https://api.github.com/users/{}/repos'\n",
        "\n",
        "    def extract_github_technical_expertise(self, github_url: str) -> int:\n",
        "        \"\"\"Extract technical expertise score from a GitHub profile.\"\"\"\n",
        "        try:\n",
        "            # Extract the GitHub username from the URL (the last part after '/')\n",
        "            github_username = github_url.split('/')[-1]\n",
        "            print(f\"Fetching data for GitHub username: {github_username}\")  # Debugging log\n",
        "\n",
        "            # Fetch the repositories using GitHub API\n",
        "            response = requests.get(self.github_api_url.format(github_username))\n",
        "\n",
        "            # Check for a valid response\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Error fetching GitHub data for {github_username}: {response.status_code}\")\n",
        "                print(f\"Response: {response.text}\")  # Debugging log to check response body\n",
        "                return 0\n",
        "\n",
        "            # Parse the response JSON to get repositories\n",
        "            repos_data = response.json()\n",
        "\n",
        "            # Check if the response is empty or contains errors\n",
        "            if isinstance(repos_data, dict) and repos_data.get('message'):\n",
        "                print(f\"GitHub API returned an error: {repos_data['message']}\")\n",
        "                return 0\n",
        "\n",
        "            # Calculate technical expertise score: Count of repositories\n",
        "            expertise_score = len(repos_data)\n",
        "\n",
        "            # Optional: You could enhance scoring by considering the number of stars or forks\n",
        "            # stars = sum([repo.get('stargazers_count', 0) for repo in repos_data])\n",
        "            # expertise_score += stars // 10  # Add bonus for stars\n",
        "\n",
        "            print(f\"Number of repositories for {github_username}: {expertise_score}\")  # Debugging log\n",
        "            return expertise_score\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting GitHub expertise: {e}\")\n",
        "            return 0  # Return 0 if there's an error\n",
        "\n",
        "    def process_csv_and_add_expertise(self, input_csv: str, output_csv: str) -> None:\n",
        "        \"\"\"Process CSV to extract technical expertise from GitHub and update CSV with the score.\"\"\"\n",
        "        with open(input_csv, mode='r', newline='', encoding='utf-8') as infile:\n",
        "            reader = csv.DictReader(infile)\n",
        "            fieldnames = reader.fieldnames + ['github_technical_expertise']\n",
        "\n",
        "            rows = []\n",
        "            for row in reader:\n",
        "                github_url = row.get('github_url')\n",
        "                if github_url:\n",
        "                    try:\n",
        "                        # Extract technical expertise score based on GitHub URL\n",
        "                        expertise_score = self.extract_github_technical_expertise(github_url)\n",
        "                        row['github_technical_expertise'] = expertise_score  # Add expertise score to the row\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing GitHub URL {github_url}: {e}\")\n",
        "                        row['github_technical_expertise'] = 0  # Default to 0 if an error occurs\n",
        "                else:\n",
        "                    row['github_technical_expertise'] = 0  # No GitHub URL provided, default to 0\n",
        "\n",
        "                rows.append(row)\n",
        "\n",
        "        # Write the updated data back to a new CSV file\n",
        "        with open(output_csv, mode='w', newline='', encoding='utf-8') as outfile:\n",
        "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(rows)\n",
        "\n",
        "        print(f\"Parsed CSV saved to {output_csv}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    parser = ResumeParser()\n",
        "\n",
        "    input_csv = '/content/extracted_data.csv'  # Replace with the actual input CSV file path\n",
        "    output_csv = '/content/extracted_data.csv'  # Replace with the desired output CSV file path\n",
        "\n",
        "    # Process the CSV to add technical expertise score from GitHub\n",
        "    parser.process_csv_and_add_expertise(input_csv, output_csv)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT3GBwYnAGuI",
        "outputId": "47e85705-ebf7-493a-e9fc-79c13af98e0e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed CSV saved to /content/extracted_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "\n",
        "class ResumeParser:\n",
        "    def __init__(self):\n",
        "        # GitHub API URL to fetch public repositories\n",
        "        self.github_api_url = 'https://api.github.com/users/{}/repos'\n",
        "\n",
        "    def extract_github_technical_expertise(self, github_url: str) -> int:\n",
        "        \"\"\"Extract technical expertise score from a GitHub profile.\"\"\"\n",
        "        try:\n",
        "            # Extract the GitHub username from the URL (the last part after '/')\n",
        "            github_username = github_url.split('/')[-1]\n",
        "            print(f\"Fetching data for GitHub username: {github_username}\")  # Debugging log\n",
        "\n",
        "            # Fetch the repositories using GitHub API\n",
        "            response = requests.get(self.github_api_url.format(github_username))\n",
        "\n",
        "            # Check for a valid response\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Error fetching GitHub data for {github_username}: {response.status_code}\")\n",
        "                print(f\"Response: {response.text}\")  # Debugging log to check response body\n",
        "                return 0\n",
        "\n",
        "            # Parse the response JSON to get repositories\n",
        "            repos_data = response.json()\n",
        "\n",
        "            # Check if the response is empty or contains errors\n",
        "            if isinstance(repos_data, dict) and repos_data.get('message'):\n",
        "                print(f\"GitHub API returned an error: {repos_data['message']}\")\n",
        "                return 0\n",
        "\n",
        "            # Calculate technical expertise score: Count of repositories\n",
        "            expertise_score = len(repos_data)\n",
        "\n",
        "            # Enhance scoring by considering the number of stars and forks\n",
        "            stars = sum([repo.get('stargazers_count', 0) for repo in repos_data])\n",
        "            forks = sum([repo.get('forks_count', 0) for repo in repos_data])\n",
        "            expertise_score += (stars // 10) + (forks // 5)  # Bonus for stars and forks\n",
        "\n",
        "            print(f\"Number of repositories for {github_username}: {len(repos_data)}\")  # Debugging log\n",
        "            print(f\"Calculated expertise score for {github_username}: {expertise_score}\")  # Debugging log\n",
        "\n",
        "            return expertise_score\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting GitHub expertise: {e}\")\n",
        "            return 0  # Return 0 if there's an error\n",
        "\n",
        "    def process_csv_and_add_expertise(self, input_csv: str, output_csv: str) -> None:\n",
        "        \"\"\"Process CSV to extract technical expertise from GitHub and update CSV with the score.\"\"\"\n",
        "        with open(input_csv, mode='r', newline='', encoding='utf-8') as infile:\n",
        "            reader = csv.DictReader(infile)\n",
        "            fieldnames = reader.fieldnames + ['github_technical_expertise']\n",
        "\n",
        "            rows = []\n",
        "            for row in reader:\n",
        "                github_url = row.get('github_url')\n",
        "                if github_url:\n",
        "                    try:\n",
        "                        # Extract technical expertise score based on GitHub URL\n",
        "                        expertise_score = self.extract_github_technical_expertise(github_url)\n",
        "                        row['github_technical_expertise'] = expertise_score  # Add expertise score to the row\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing GitHub URL {github_url}: {e}\")\n",
        "                        row['github_technical_expertise'] = 0  # Default to 0 if an error occurs\n",
        "                else:\n",
        "                    row['github_technical_expertise'] = 0  # No GitHub URL provided, default to 0\n",
        "\n",
        "                rows.append(row)\n",
        "\n",
        "        # Write the updated data back to a new CSV file\n",
        "        with open(output_csv, mode='w', newline='', encoding='utf-8') as outfile:\n",
        "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(rows)\n",
        "\n",
        "        print(f\"Parsed CSV saved to {output_csv}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    parser = ResumeParser()\n",
        "\n",
        "    input_csv = '/content/extracted_data.csv'  # Replace with the actual input CSV file path\n",
        "    output_csv = '/content/extracted_data.csv'  # Replace with the desired output CSV file path\n",
        "\n",
        "    # Process the CSV to add technical expertise score from GitHub\n",
        "    parser.process_csv_and_add_expertise(input_csv, output_csv)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTZm8020dVM0",
        "outputId": "64196d9f-46e9-4aa7-ad4d-4587df80eb22"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed CSV saved to /content/extracted_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pdfminer.six\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMvXGJXXh9c-",
        "outputId": "d45a1e08-5218-4496-f0a7-d60df10456ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20240706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "TycfmBYWiD9o",
        "outputId": "3d64b077-e71a-44b3-f23b-52dcec31b8f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pdfminer3'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c8f405a9a507>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meverygrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLAParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdfpage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPDFPage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpdfminer3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdfinterp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPDFResourceManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pdfminer3'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def extract_github_urls_from_csv(input_csv):\n",
        "    \"\"\"\n",
        "    Extract GitHub URLs from the input CSV file.\n",
        "    \"\"\"\n",
        "    github_urls = []\n",
        "    with open(input_csv, mode='r', newline='', encoding='utf-8') as infile:\n",
        "        reader = csv.DictReader(infile)\n",
        "        for row in reader:\n",
        "            github_url = row.get('github_url')\n",
        "            if github_url and 'github.com' in github_url:\n",
        "                github_urls.append((row, github_url))  # Store the row and URL\n",
        "    return github_urls\n",
        "\n",
        "def extract_github_features(github_url, token):\n",
        "    \"\"\"\n",
        "    Extract features from a GitHub profile using the GitHub API.\n",
        "    \"\"\"\n",
        "    username = github_url.rstrip(\"/\").split(\"/\")[-1]\n",
        "    headers = {\"Authorization\": f\"token {token}\"}\n",
        "\n",
        "    # API Endpoints\n",
        "    user_url = f\"https://api.github.com/users/{username}\"\n",
        "    repos_url = f\"https://api.github.com/users/{username}/repos\"\n",
        "\n",
        "    try:\n",
        "        # Fetch user details\n",
        "        user_response = requests.get(user_url, headers=headers)\n",
        "        user_data = user_response.json()\n",
        "\n",
        "        # Fetch repository details\n",
        "        repos_response = requests.get(repos_url, headers=headers)\n",
        "        repos_data = repos_response.json()\n",
        "\n",
        "        features = {\n",
        "            \"username\": user_data.get(\"login\"),\n",
        "            \"public_repos_count\": user_data.get(\"public_repos\"),\n",
        "            \"followers\": user_data.get(\"followers\"),\n",
        "            \"languages\": [],\n",
        "            \"stars\": 0,\n",
        "            \"community_interaction\": 0\n",
        "        }\n",
        "\n",
        "        for repo in repos_data:\n",
        "            features[\"languages\"].append(repo.get(\"language\"))\n",
        "            features[\"stars\"] += repo.get(\"stargazers_count\", 0)\n",
        "            if repo.get(\"pulls_url\"):\n",
        "                features[\"community_interaction\"] += 1\n",
        "\n",
        "        # Deduplicate languages\n",
        "        features[\"languages\"] = list(set(features[\"languages\"]))\n",
        "        return features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def calculate_technical_score(features):\n",
        "    \"\"\"\n",
        "    Calculate the technical score based on GitHub data.\n",
        "    \"\"\"\n",
        "    code_quality_score = 0.80\n",
        "    tech_stack_score = min(1, len(features[\"languages\"]) / 10)\n",
        "    project_complexity_score = min(features[\"stars\"] / 500, 1)\n",
        "    contribution_score = min(features[\"community_interaction\"] / 10, 1)\n",
        "    testing_doc_score = 0.90\n",
        "\n",
        "    technical_score = (\n",
        "        (code_quality_score * 0.30) +\n",
        "        (tech_stack_score * 0.25) +\n",
        "        (project_complexity_score * 0.25) +\n",
        "        (contribution_score * 0.10) +\n",
        "        (testing_doc_score * 0.10)\n",
        "    )\n",
        "    return round(technical_score * 100, 2)\n",
        "\n",
        "def process_csv_and_update_scores(input_csv, output_csv, token):\n",
        "    \"\"\"\n",
        "    Process the CSV to calculate and update technical expertise scores for GitHub URLs.\n",
        "    \"\"\"\n",
        "    updated_rows = []\n",
        "    with open(input_csv, mode='r', newline='', encoding='utf-8') as infile:\n",
        "        reader = csv.DictReader(infile)\n",
        "        fieldnames = reader.fieldnames + ['github_technical_expertise']\n",
        "\n",
        "        for row in reader:\n",
        "            github_url = row.get('github_url')\n",
        "            if github_url and 'github.com' in github_url:\n",
        "                features = extract_github_features(github_url, token)\n",
        "                if features:\n",
        "                    expertise_score = calculate_technical_score(features)\n",
        "                    row['github_technical_expertise'] = expertise_score\n",
        "                else:\n",
        "                    row['github_technical_expertise'] = 0\n",
        "            else:\n",
        "                row['github_technical_expertise'] = 0\n",
        "\n",
        "            updated_rows.append(row)\n",
        "\n",
        "    # Write updated data to a new CSV file\n",
        "    with open(output_csv, mode='w', newline='', encoding='utf-8') as outfile:\n",
        "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(updated_rows)\n",
        "\n",
        "    print(f\"Updated CSV saved to {output_csv}\")\n",
        "\n",
        "def main():\n",
        "    input_csv = \"/content/extracted_data.csv\"  # Replace with your input CSV file path\n",
        "    output_csv = \"/content/extracted_data.csv\"  # Replace with your output CSV file path\n",
        "    token = \"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\"  # Replace with your GitHub token\n",
        "\n",
        "    # Process CSV to calculate and update technical expertise scores\n",
        "    process_csv_and_update_scores(input_csv, output_csv, token)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH1SSmvBjWO1",
        "outputId": "97b2ea24-d1fd-4579-91e0-728b5a6d74e7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated CSV saved to /content/extracted_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymupdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeqG93BOkk77",
        "outputId": "6a45835c-2862-451f-9efc-d90406196a8f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.1-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def extract_github_features(github_url, token):\n",
        "    \"\"\"\n",
        "    Extract features from a GitHub profile using the GitHub API.\n",
        "    \"\"\"\n",
        "    # Remove trailing slash and get username\n",
        "    username = github_url.rstrip(\"/\").split(\"/\")[-1]\n",
        "    headers = {\"Authorization\": f\"token {token}\"}\n",
        "\n",
        "    # API Endpoints\n",
        "    user_url = f\"https://api.github.com/users/{username}\"\n",
        "    repos_url = f\"https://api.github.com/users/{username}/repos\"\n",
        "\n",
        "    try:\n",
        "        # Fetch user details\n",
        "        user_response = requests.get(user_url, headers=headers)\n",
        "        user_data = user_response.json()\n",
        "\n",
        "        # Fetch repository details\n",
        "        repos_response = requests.get(repos_url, headers=headers)\n",
        "        repos_data = repos_response.json()\n",
        "\n",
        "        features = {\n",
        "            \"username\": user_data.get(\"login\"),\n",
        "            \"name\": user_data.get(\"name\"),\n",
        "            \"bio\": user_data.get(\"bio\"),\n",
        "            \"location\": user_data.get(\"location\"),\n",
        "            \"public_repos_count\": user_data.get(\"public_repos\"),\n",
        "            \"followers\": user_data.get(\"followers\"),\n",
        "            \"following\": user_data.get(\"following\"),\n",
        "            \"top_repositories\": [],\n",
        "            \"languages\": [],\n",
        "            \"commit_count\": 0,  # To track total commits\n",
        "            \"community_interaction\": 0  # For pull requests and issues\n",
        "        }\n",
        "\n",
        "        # Analyze repositories to get top languages and popular repositories\n",
        "        for repo in repos_data:\n",
        "            repo_details = {\n",
        "                \"name\": repo.get(\"name\"),\n",
        "                \"stars\": repo.get(\"stargazers_count\"),\n",
        "                \"forks\": repo.get(\"forks_count\"),\n",
        "                \"url\": repo.get(\"html_url\"),\n",
        "                \"language\": repo.get(\"language\"),\n",
        "                \"created_at\": repo.get(\"created_at\"),\n",
        "                \"updated_at\": repo.get(\"updated_at\"),\n",
        "            }\n",
        "\n",
        "            # If the repo's language is 'Jupyter Notebook', treat it as Python (common in data science repos)\n",
        "            if repo.get(\"language\") == \"Jupyter Notebook\":\n",
        "                repo_details[\"language\"] = \"Python\"\n",
        "\n",
        "            features[\"top_repositories\"].append(repo_details)\n",
        "\n",
        "            # Append the language to the languages list, excluding 'Jupyter Notebook' for clearer representation\n",
        "            if repo.get(\"language\") and repo[\"language\"] != \"Jupyter Notebook\":\n",
        "                features[\"languages\"].append(repo.get(\"language\"))\n",
        "\n",
        "            # Count commits (simplified for this example)\n",
        "            features[\"commit_count\"] += repo.get(\"stargazers_count\", 0)  # Using stars as proxy for commits\n",
        "\n",
        "            # Check community interaction (pull requests, issues)\n",
        "            if repo.get(\"pulls_url\"):\n",
        "                features[\"community_interaction\"] += 1  # Increment if there is a pull request URL\n",
        "\n",
        "        # Sort repositories by stars and take top 5\n",
        "        features[\"top_repositories\"] = sorted(features[\"top_repositories\"], key=lambda x: x[\"stars\"], reverse=True)[:5]\n",
        "\n",
        "        return features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {username}: {e}\")\n",
        "        return None\n",
        "\n",
        "def calculate_technical_score(features):\n",
        "    \"\"\"\n",
        "    Calculate the technical score based on GitHub data.\n",
        "    \"\"\"\n",
        "    # Handling case where no features were extracted\n",
        "    if not features:\n",
        "        return 0\n",
        "\n",
        "    # Code Quality (30%): Simulated score (you can use tools like SonarQube for a more accurate score)\n",
        "    code_quality_score = 0.80  # Assuming 80% of the maximum (from SonarQube, etc.)\n",
        "\n",
        "    # Technical Skills (25%): Based on number of languages used and relevance of tech stack\n",
        "    tech_stack_score = min(1, len(set(features[\"languages\"])) / 10)  # Score based on languages used, max 10 languages\n",
        "    tech_stack_score = 0.90 if tech_stack_score > 0.9 else tech_stack_score  # Adjust based on relevance\n",
        "\n",
        "    # Project Complexity (25%): Based on repository stars and size\n",
        "    project_complexity_score = sum([repo[\"stars\"] for repo in features[\"top_repositories\"]]) / 500  # Max 500 stars across top repos\n",
        "    project_complexity_score = min(project_complexity_score, 1)  # Scale to 0-1\n",
        "\n",
        "    # Contributions & Collaboration (10%): Number of contributions (pull requests, issues)\n",
        "    contribution_score = features[\"community_interaction\"] / 10  # Max 10 contributions (can scale this as needed)\n",
        "\n",
        "    # Testing & Documentation (10%): Simulated score\n",
        "    testing_doc_score = 0.90  # Assuming full coverage and documentation (can use tools like Codacy)\n",
        "\n",
        "    # Total score calculation\n",
        "    technical_score = (code_quality_score * 0.30) + (tech_stack_score * 0.25) + \\\n",
        "                      (project_complexity_score * 0.25) + (contribution_score * 0.10) + \\\n",
        "                      (testing_doc_score * 0.10)\n",
        "\n",
        "    return technical_score\n",
        "\n",
        "def plot_technical_scores(scores):\n",
        "    \"\"\"\n",
        "    Plot a bar chart for technical scores of multiple profiles.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    usernames = [score[\"name\"] for score in scores]\n",
        "    technical_scores = [score[\"score\"] * 100 for score in scores]  # Convert to percentage\n",
        "\n",
        "    plt.bar(usernames, technical_scores, color='blue')\n",
        "    plt.ylim(0, 100)\n",
        "    plt.ylabel('Technical Expertise Score (%)')\n",
        "    plt.title('Technical Expertise Scores of GitHub Profiles')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('technical_expertise_scores.png')\n",
        "    plt.close()\n",
        "\n",
        "def display_user_summary(features, technical_score, output_file):\n",
        "    \"\"\"\n",
        "    Display summary of GitHub profile features along with technical score.\n",
        "    Write to an output file for comprehensive reporting.\n",
        "    \"\"\"\n",
        "    with open(output_file, 'a') as f:\n",
        "        f.write(\"\\nGitHub Profile Summary:\\n\")\n",
        "        f.write(\"------------------------\\n\")\n",
        "        f.write(f\"Username: {features['username']}\\n\")\n",
        "        f.write(f\"Full Name: {features['name']}\\n\")\n",
        "        f.write(f\"Bio: {features['bio']}\\n\")\n",
        "        f.write(f\"Location: {features['location']}\\n\")\n",
        "        f.write(f\"Public Repositories: {features['public_repos_count']}\\n\")\n",
        "        f.write(f\"Followers: {features['followers']}\\n\")\n",
        "        f.write(f\"Following: {features['following']}\\n\")\n",
        "        f.write(f\"Technical Expertise Score: {technical_score * 100:.2f}/100\\n\")\n",
        "\n",
        "        f.write(\"\\nTop Repositories (by Stars):\\n\")\n",
        "        f.write(\"-----------------------------\\n\")\n",
        "        for repo in features['top_repositories']:\n",
        "            f.write(f\"Repo Name: {repo['name']}\\n\")\n",
        "            f.write(f\"Stars: {repo['stars']}\\n\")\n",
        "            f.write(f\"Forks: {repo['forks']}\\n\")\n",
        "            f.write(f\"Language: {repo['language']}\\n\")\n",
        "            f.write(f\"URL: {repo['url']}\\n\")\n",
        "            f.write(\"-----------------------------\\n\")\n",
        "\n",
        "def main(csv_path, github_token, output_path='github_expertise_analysis.txt'):\n",
        "    \"\"\"\n",
        "    Main function to process GitHub links from a CSV file.\n",
        "\n",
        "    :param csv_path: Path to the CSV file containing GitHub links\n",
        "    :param github_token: GitHub Personal Access Token\n",
        "    :param output_path: Path to save the output analysis\n",
        "    \"\"\"\n",
        "    # Read the CSV file\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "        return\n",
        "\n",
        "    # Assume the CSV has a column named 'github_link'\n",
        "    # If the column has a different name, modify accordingly\n",
        "    github_column = 'github'\n",
        "\n",
        "    # Validate the column exists\n",
        "    if github_column not in df.columns:\n",
        "        print(f\"Error: No column named '{github_column}' found in the CSV.\")\n",
        "        return\n",
        "\n",
        "    # Clear previous output file\n",
        "    open(output_path, 'w').close()\n",
        "\n",
        "    # Store scores for plotting\n",
        "    all_scores = []\n",
        "\n",
        "    # Process each GitHub link\n",
        "    for index, row in df.iterrows():\n",
        "        github_url = row[github_column]\n",
        "\n",
        "        try:\n",
        "            # Extract features\n",
        "            features = extract_github_features(github_url, github_token)\n",
        "\n",
        "            if features:\n",
        "                # Calculate technical score\n",
        "                technical_score = calculate_technical_score(features)\n",
        "\n",
        "                # Store the score along with the name for ranking\n",
        "                all_scores.append({\n",
        "                    \"name\": features[\"name\"] or features[\"username\"],\n",
        "                    \"score\": technical_score\n",
        "                })\n",
        "\n",
        "                # Display and save user summary\n",
        "                display_user_summary(features, technical_score, output_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {github_url}: {e}\")\n",
        "\n",
        "    # Sort profiles by score in descending order\n",
        "    all_scores.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "    # Plotting the technical scores as a bar chart\n",
        "    plot_technical_scores(all_scores)\n",
        "\n",
        "    print(f\"Analysis complete. Results saved to {output_path}\")\n",
        "    print(\"Technical expertise scores chart saved as technical_expertise_scores.png\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace these with your actual values\n",
        "    CSV_FILE_PATH = '/content/extracted_data.csv'  # Path to your CSV file\n",
        "    GITHUB_TOKEN = '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'  # Replace with your GitHub token\n",
        "\n",
        "    main(CSV_FILE_PATH, GITHUB_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN84WlZ_mE-A",
        "outputId": "73d99e6b-0586-41df-aada-a48a8768a509"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis complete. Results saved to github_expertise_analysis.txt\n",
            "Technical expertise scores chart saved as technical_expertise_scores.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def extract_github_features(github_url, token):\n",
        "    \"\"\"\n",
        "    Extract features from a GitHub profile using the GitHub API.\n",
        "    \"\"\"\n",
        "    # Remove trailing slash and get username\n",
        "    username = github_url.rstrip(\"/\").split(\"/\")[-1]\n",
        "    headers = {\"Authorization\": f\"token {token}\"}\n",
        "\n",
        "    # API Endpoints\n",
        "    user_url = f\"https://api.github.com/users/{username}\"\n",
        "    repos_url = f\"https://api.github.com/users/{username}/repos\"\n",
        "\n",
        "    try:\n",
        "        # Fetch user details\n",
        "        user_response = requests.get(user_url, headers=headers)\n",
        "        user_data = user_response.json()\n",
        "\n",
        "        # Fetch repository details\n",
        "        repos_response = requests.get(repos_url, headers=headers)\n",
        "        repos_data = repos_response.json()\n",
        "\n",
        "        features = {\n",
        "            \"username\": user_data.get(\"login\"),\n",
        "            \"name\": user_data.get(\"name\"),\n",
        "            \"public_repos_count\": user_data.get(\"public_repos\", 0),\n",
        "            \"top_repositories\": [],\n",
        "            \"languages\": [],\n",
        "            \"commit_count\": 0,\n",
        "            \"community_interaction\": 0\n",
        "        }\n",
        "\n",
        "        # Analyze repositories\n",
        "        for repo in repos_data:\n",
        "            # Skip if repo details are None\n",
        "            if not repo:\n",
        "                continue\n",
        "\n",
        "            # Repository details\n",
        "            repo_details = {\n",
        "                \"name\": repo.get(\"name\"),\n",
        "                \"stars\": repo.get(\"stargazers_count\", 0),\n",
        "                \"forks\": repo.get(\"forks_count\", 0),\n",
        "                \"language\": repo.get(\"language\")\n",
        "            }\n",
        "\n",
        "            # If the repo's language is 'Jupyter Notebook', treat it as Python\n",
        "            if repo.get(\"language\") == \"Jupyter Notebook\":\n",
        "                repo_details[\"language\"] = \"Python\"\n",
        "\n",
        "            features[\"top_repositories\"].append(repo_details)\n",
        "\n",
        "            # Collect unique languages\n",
        "            if repo.get(\"language\") and repo[\"language\"] != \"Jupyter Notebook\":\n",
        "                features[\"languages\"].append(repo.get(\"language\"))\n",
        "\n",
        "            # Count commits (using stars as a proxy)\n",
        "            features[\"commit_count\"] += repo.get(\"stargazers_count\", 0)\n",
        "\n",
        "            # Check community interaction\n",
        "            if repo.get(\"pulls_url\"):\n",
        "                features[\"community_interaction\"] += 1\n",
        "\n",
        "        # Sort repositories by stars and take top 5\n",
        "        features[\"top_repositories\"] = sorted(features[\"top_repositories\"], key=lambda x: x[\"stars\"], reverse=True)[:5]\n",
        "\n",
        "        return features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {username}: {e}\")\n",
        "        return None\n",
        "\n",
        "def calculate_technical_score(features):\n",
        "    \"\"\"\n",
        "    Calculate the technical score based on GitHub data.\n",
        "    \"\"\"\n",
        "    # Handling case where no features were extracted\n",
        "    if not features or not features.get(\"top_repositories\"):\n",
        "        return 0\n",
        "\n",
        "    # Code Quality (30%)\n",
        "    code_quality_score = 0.80\n",
        "\n",
        "    # Technical Skills (25%): Based on number of languages used\n",
        "    tech_stack_score = min(1, len(set(features[\"languages\"])) / 10)\n",
        "    tech_stack_score = 0.90 if tech_stack_score > 0.9 else tech_stack_score\n",
        "\n",
        "    # Project Complexity (25%): Based on repository stars\n",
        "    project_complexity_score = sum([repo[\"stars\"] for repo in features[\"top_repositories\"]]) / 500\n",
        "    project_complexity_score = min(project_complexity_score, 1)\n",
        "\n",
        "    # Contributions & Collaboration (10%)\n",
        "    contribution_score = features[\"community_interaction\"] / 10\n",
        "\n",
        "    # Testing & Documentation (10%)\n",
        "    testing_doc_score = 0.90\n",
        "\n",
        "    # Total score calculation\n",
        "    technical_score = (\n",
        "        (code_quality_score * 0.30) +\n",
        "        (tech_stack_score * 0.25) +\n",
        "        (project_complexity_score * 0.25) +\n",
        "        (contribution_score * 0.10) +\n",
        "        (testing_doc_score * 0.10)\n",
        "    )\n",
        "\n",
        "    return technical_score\n",
        "\n",
        "def process_github_links(csv_path, github_token):\n",
        "    \"\"\"\n",
        "    Process GitHub links from CSV and calculate technical expertise scores.\n",
        "\n",
        "    :param csv_path: Path to the CSV file containing GitHub links\n",
        "    :param github_token: GitHub Personal Access Token\n",
        "    :return: Updated DataFrame with technical expertise scores\n",
        "    \"\"\"\n",
        "    # Read the CSV file\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Assume the CSV has a column named 'github_link'\n",
        "    # If the column has a different name, modify accordingly\n",
        "    github_column = 'github'\n",
        "\n",
        "    # Validate the column exists\n",
        "    if github_column not in df.columns:\n",
        "        print(f\"Error: No column named '{github_column}' found in the CSV.\")\n",
        "        return None\n",
        "\n",
        "    # Add a new column for technical expertise score\n",
        "    df['technical_expertise_score'] = 0.0\n",
        "\n",
        "    # Process each GitHub link\n",
        "    for index, row in df.iterrows():\n",
        "        github_url = row[github_column]\n",
        "\n",
        "        try:\n",
        "            # Extract features\n",
        "            features = extract_github_features(github_url, github_token)\n",
        "\n",
        "            if features:\n",
        "                # Calculate technical score\n",
        "                technical_score = calculate_technical_score(features)\n",
        "\n",
        "                # Store the score in the DataFrame\n",
        "                df.at[index, 'technical_expertise_score'] = round(technical_score * 100, 2)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {github_url}: {e}\")\n",
        "            df.at[index, 'technical_expertise_score'] = 0.0\n",
        "\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    # Replace these with your actual values\n",
        "    CSV_FILE_PATH = '/content/extracted_data.csv'  # Path to your CSV file\n",
        "    GITHUB_TOKEN = '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'  # Replace with your GitHub token\n",
        "    OUTPUT_FILE_PATH = '/content/extracted_data.csv'  # Output CSV file path\n",
        "\n",
        "    # Process GitHub links and calculate scores\n",
        "    updated_df = process_github_links(CSV_FILE_PATH, GITHUB_TOKEN)\n",
        "\n",
        "    if updated_df is not None:\n",
        "        # Save the updated DataFrame to a new CSV\n",
        "        updated_df.to_csv(OUTPUT_FILE_PATH, index=False)\n",
        "        print(f\"Analysis complete. Results saved to {OUTPUT_FILE_PATH}\")\n",
        "\n",
        "        # Print top 5 profiles by technical expertise score\n",
        "        print(\"\\nTop 5 Profiles by Technical Expertise Score:\")\n",
        "        top_profiles = updated_df.nlargest(5, 'technical_expertise_score')\n",
        "        print(top_profiles[['github', 'technical_expertise_score']])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im5mdYnVnKC3",
        "outputId": "98aec716-df7f-4036-f6e8-4a529a56c3da"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis complete. Results saved to /content/extracted_data.csv\n",
            "\n",
            "Top 5 Profiles by Technical Expertise Score:\n",
            "                               github  technical_expertise_score\n",
            "3  https://gist.github.com/kentcdodds                       98.0\n",
            "4     https://github.com/jessitronica                       73.0\n",
            "1         https://github.com/torvalds                       70.0\n",
            "2     https://github.com/julia-stripe                       62.0\n",
            "0  https://github.com/saikruthikreddy                       40.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def extract_github_features(github_url, token):\n",
        "    \"\"\"\n",
        "    Extract features from a GitHub profile using the GitHub API.\n",
        "    \"\"\"\n",
        "    username = github_url.rstrip(\"/\").split(\"/\")[-1]\n",
        "    headers = {\"Authorization\": f\"token {token}\"}\n",
        "\n",
        "    user_url = f\"https://api.github.com/users/{username}\"\n",
        "    repos_url = f\"https://api.github.com/users/{username}/repos\"\n",
        "\n",
        "    try:\n",
        "        user_response = requests.get(user_url, headers=headers)\n",
        "        user_data = user_response.json()\n",
        "\n",
        "        repos_response = requests.get(repos_url, headers=headers)\n",
        "        repos_data = repos_response.json()\n",
        "\n",
        "        features = {\n",
        "            \"username\": user_data.get(\"login\"),\n",
        "            \"name\": user_data.get(\"name\"),\n",
        "            \"public_repos_count\": user_data.get(\"public_repos\", 0),\n",
        "            \"top_repositories\": [],\n",
        "            \"languages\": [],\n",
        "            \"commit_count\": 0,\n",
        "            \"community_interaction\": 0\n",
        "        }\n",
        "\n",
        "        for repo in repos_data:\n",
        "            if not repo:\n",
        "                continue\n",
        "            repo_details = {\n",
        "                \"name\": repo.get(\"name\"),\n",
        "                \"stars\": repo.get(\"stargazers_count\", 0),\n",
        "                \"forks\": repo.get(\"forks_count\", 0),\n",
        "                \"language\": repo.get(\"language\")\n",
        "            }\n",
        "            if repo.get(\"language\") == \"Jupyter Notebook\":\n",
        "                repo_details[\"language\"] = \"Python\"\n",
        "\n",
        "            features[\"top_repositories\"].append(repo_details)\n",
        "            if repo.get(\"language\"):\n",
        "                features[\"languages\"].append(repo.get(\"language\"))\n",
        "            features[\"commit_count\"] += repo.get(\"stargazers_count\", 0)\n",
        "            if repo.get(\"pulls_url\"):\n",
        "                features[\"community_interaction\"] += 1\n",
        "\n",
        "        features[\"top_repositories\"] = sorted(\n",
        "            features[\"top_repositories\"], key=lambda x: x[\"stars\"], reverse=True\n",
        "        )[:5]\n",
        "\n",
        "        return features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {github_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def calculate_technical_score(features):\n",
        "    if not features or not features.get(\"top_repositories\"):\n",
        "        return 0\n",
        "\n",
        "    code_quality_score = 0.80\n",
        "    tech_stack_score = min(1, len(set(features[\"languages\"])) / 10)\n",
        "    project_complexity_score = sum([repo[\"stars\"] for repo in features[\"top_repositories\"]]) / 500\n",
        "    project_complexity_score = min(project_complexity_score, 1)\n",
        "    contribution_score = features[\"community_interaction\"] / 10\n",
        "    testing_doc_score = 0.90\n",
        "\n",
        "    technical_score = (\n",
        "        (code_quality_score * 0.30) +\n",
        "        (tech_stack_score * 0.25) +\n",
        "        (project_complexity_score * 0.25) +\n",
        "        (contribution_score * 0.10) +\n",
        "        (testing_doc_score * 0.10)\n",
        "    )\n",
        "\n",
        "    return round(technical_score * 100, 2)\n",
        "\n",
        "def process_github_links(csv_path, github_token):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "        return None\n",
        "\n",
        "    github_column = 'github'\n",
        "\n",
        "    if github_column not in df.columns:\n",
        "        print(f\"Error: No column named '{github_column}' found in the CSV.\")\n",
        "        return None\n",
        "\n",
        "    df['technical_expertise_score'] = 0.0\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        github_url = row[github_column]\n",
        "        if pd.notna(github_url):\n",
        "            try:\n",
        "                features = extract_github_features(github_url, github_token)\n",
        "                if features:\n",
        "                    technical_score = calculate_technical_score(features)\n",
        "                    df.at[index, 'technical_expertise_score'] = technical_score\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {github_url}: {e}\")\n",
        "                df.at[index, 'technical_expertise_score'] = 0.0\n",
        "\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    CSV_FILE_PATH = '/content/extracted_data.csv'\n",
        "    GITHUB_TOKEN = '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'\n",
        "    OUTPUT_FILE_PATH = '/content/extracted_data.csv'\n",
        "\n",
        "    updated_df = process_github_links(CSV_FILE_PATH, GITHUB_TOKEN)\n",
        "\n",
        "    if updated_df is not None:\n",
        "        updated_df.to_csv(OUTPUT_FILE_PATH, index=False)\n",
        "        print(f\"Analysis complete. Results saved to {OUTPUT_FILE_PATH}\")\n",
        "        print(\"\\nTop 5 Profiles by Technical Expertise Score:\")\n",
        "        top_profiles = updated_df.nlargest(5, 'technical_expertise_score')\n",
        "        print(top_profiles[['github', 'technical_expertise_score']])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIZLNqenqeye",
        "outputId": "8f10182c-31f2-4fc9-fc94-aa4c097d31f6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis complete. Results saved to /content/extracted_data.csv\n",
            "\n",
            "Top 5 Profiles by Technical Expertise Score:\n",
            "                               github  technical_expertise_score\n",
            "3  https://gist.github.com/kentcdodds                       98.0\n",
            "4     https://github.com/jessitronica                       73.0\n",
            "1         https://github.com/torvalds                       70.0\n",
            "2     https://github.com/julia-stripe                       62.0\n",
            "0  https://github.com/saikruthikreddy                       43.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def extract_github_features(github_url, token):\n",
        "    \"\"\"\n",
        "    Extract features from a GitHub profile using the GitHub API.\n",
        "\n",
        "    Args:\n",
        "        github_url (str): URL of the GitHub profile\n",
        "        token (str): GitHub API authentication token\n",
        "\n",
        "    Returns:\n",
        "        dict: Extracted features from the GitHub profile\n",
        "    \"\"\"\n",
        "    username = github_url.rstrip(\"/\").split(\"/\")[-1]\n",
        "    headers = {\"Authorization\": f\"token {token}\"}\n",
        "\n",
        "    user_url = f\"https://api.github.com/users/{username}\"\n",
        "    repos_url = f\"https://api.github.com/users/{username}/repos\"\n",
        "\n",
        "    try:\n",
        "        user_response = requests.get(user_url, headers=headers)\n",
        "        user_data = user_response.json()\n",
        "\n",
        "        repos_response = requests.get(repos_url, headers=headers)\n",
        "        repos_data = repos_response.json()\n",
        "\n",
        "        features = {\n",
        "            \"username\": user_data.get(\"login\"),\n",
        "            \"name\": user_data.get(\"name\"),\n",
        "            \"public_repos_count\": user_data.get(\"public_repos\", 0),\n",
        "            \"top_repositories\": [],\n",
        "            \"languages\": [],\n",
        "            \"commit_count\": 0,\n",
        "            \"community_interaction\": 0\n",
        "        }\n",
        "\n",
        "        for repo in repos_data:\n",
        "            if not repo:\n",
        "                continue\n",
        "            repo_details = {\n",
        "                \"name\": repo.get(\"name\"),\n",
        "                \"stars\": repo.get(\"stargazers_count\", 0),\n",
        "                \"forks\": repo.get(\"forks_count\", 0),\n",
        "                \"language\": repo.get(\"language\")\n",
        "            }\n",
        "            if repo.get(\"language\") == \"Jupyter Notebook\":\n",
        "                repo_details[\"language\"] = \"Python\"\n",
        "\n",
        "            features[\"top_repositories\"].append(repo_details)\n",
        "            if repo.get(\"language\"):\n",
        "                features[\"languages\"].append(repo.get(\"language\"))\n",
        "            features[\"commit_count\"] += repo.get(\"stargazers_count\", 0)\n",
        "            if repo.get(\"pulls_url\"):\n",
        "                features[\"community_interaction\"] += 1\n",
        "\n",
        "        features[\"top_repositories\"] = sorted(\n",
        "            features[\"top_repositories\"], key=lambda x: x[\"stars\"], reverse=True\n",
        "        )[:5]\n",
        "\n",
        "        return features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {github_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def calculate_technical_score(features):\n",
        "    \"\"\"\n",
        "    Calculate technical expertise score based on GitHub profile features.\n",
        "\n",
        "    Args:\n",
        "        features (dict): Features extracted from GitHub profile\n",
        "\n",
        "    Returns:\n",
        "        float: Technical expertise score (0-100)\n",
        "    \"\"\"\n",
        "    if not features or not features.get(\"top_repositories\"):\n",
        "        return 0\n",
        "\n",
        "    code_quality_score = 0.80\n",
        "    tech_stack_score = min(1, len(set(features[\"languages\"])) / 10)\n",
        "    project_complexity_score = sum([repo[\"stars\"] for repo in features[\"top_repositories\"]]) / 500\n",
        "    project_complexity_score = min(project_complexity_score, 1)\n",
        "    contribution_score = features[\"community_interaction\"] / 10\n",
        "    testing_doc_score = 0.90\n",
        "\n",
        "    technical_score = (\n",
        "        (code_quality_score * 0.30) +\n",
        "        (tech_stack_score * 0.25) +\n",
        "        (project_complexity_score * 0.25) +\n",
        "        (contribution_score * 0.10) +\n",
        "        (testing_doc_score * 0.10)\n",
        "    )\n",
        "\n",
        "    return round(technical_score * 100, 2)\n",
        "\n",
        "def process_github_links(csv_path, github_token):\n",
        "    \"\"\"\n",
        "    Process GitHub links in a CSV file and calculate technical expertise scores.\n",
        "\n",
        "    Args:\n",
        "        csv_path (str): Path to the input CSV file\n",
        "        github_token (str): GitHub API authentication token\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Updated DataFrame with technical expertise scores\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "        return None\n",
        "\n",
        "    github_column = 'github'\n",
        "\n",
        "    if github_column not in df.columns:\n",
        "        print(f\"Error: No column named '{github_column}' found in the CSV.\")\n",
        "        return None\n",
        "\n",
        "    df['technical_expertise_score'] = 0.0\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        github_url = row[github_column]\n",
        "        if pd.notna(github_url):\n",
        "            try:\n",
        "                features = extract_github_features(github_url, github_token)\n",
        "                if features:\n",
        "                    technical_score = calculate_technical_score(features)\n",
        "                    df.at[index, 'technical_expertise_score'] = technical_score\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {github_url}: {e}\")\n",
        "                df.at[index, 'technical_expertise_score'] = 0.0\n",
        "\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the GitHub profile analysis.\n",
        "    \"\"\"\n",
        "    # Update these paths and token according to your environment\n",
        "    CSV_FILE_PATH = '/content/extracted_data_with_promotions.csv'\n",
        "    GITHUB_TOKEN = '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'\n",
        "    OUTPUT_FILE_PATH = '/content/extracted_data_with_promotions.csv'\n",
        "\n",
        "    updated_df = process_github_links(CSV_FILE_PATH, GITHUB_TOKEN)\n",
        "\n",
        "    if updated_df is not None:\n",
        "        updated_df.to_csv(OUTPUT_FILE_PATH, index=False)\n",
        "        print(f\"Analysis complete. Results saved to {OUTPUT_FILE_PATH}\")\n",
        "        print(\"\\nTop 5 Profiles by Technical Expertise Score:\")\n",
        "        top_profiles = updated_df.nlargest(5, 'technical_expertise_score')\n",
        "        print(top_profiles[['github', 'technical_expertise_score']])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7a41Ht_sOfS",
        "outputId": "916b4402-9a1c-46d8-b59f-c4309c15fe38"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis complete. Results saved to /content/extracted_data_with_promotions.csv\n",
            "\n",
            "Top 5 Profiles by Technical Expertise Score:\n",
            "                               github  technical_expertise_score\n",
            "4  https://gist.github.com/kentcdodds                       98.0\n",
            "5     https://github.com/jessitronica                       73.0\n",
            "2         https://github.com/torvalds                       70.0\n",
            "3     https://github.com/julia-stripe                       62.0\n",
            "1       https://github.com/amiradridi                       54.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def extract_github_features(github_url, token):\n",
        "    \"\"\"\n",
        "    Extract features from a GitHub profile using the GitHub API.\n",
        "\n",
        "    Args:\n",
        "        github_url (str): URL of the GitHub profile\n",
        "        token (str): GitHub API authentication token\n",
        "\n",
        "    Returns:\n",
        "        dict: Extracted features from the GitHub profile\n",
        "    \"\"\"\n",
        "    username = github_url.rstrip(\"/\").split(\"/\")[-1]\n",
        "    headers = {\"Authorization\": f\"token {token}\"}\n",
        "\n",
        "    user_url = f\"https://api.github.com/users/{username}\"\n",
        "    repos_url = f\"https://api.github.com/users/{username}/repos\"\n",
        "\n",
        "    try:\n",
        "        user_response = requests.get(user_url, headers=headers)\n",
        "        user_data = user_response.json()\n",
        "\n",
        "        repos_response = requests.get(repos_url, headers=headers)\n",
        "        repos_data = repos_response.json()\n",
        "\n",
        "        features = {\n",
        "            \"username\": user_data.get(\"login\"),\n",
        "            \"name\": user_data.get(\"name\"),\n",
        "            \"public_repos_count\": user_data.get(\"public_repos\", 0),\n",
        "            \"top_repositories\": [],\n",
        "            \"languages\": [],\n",
        "            \"commit_count\": 0,\n",
        "            \"community_interaction\": 0\n",
        "        }\n",
        "\n",
        "        for repo in repos_data:\n",
        "            if not repo:\n",
        "                continue\n",
        "            repo_details = {\n",
        "                \"name\": repo.get(\"name\"),\n",
        "                \"stars\": repo.get(\"stargazers_count\", 0),\n",
        "                \"forks\": repo.get(\"forks_count\", 0),\n",
        "                \"language\": repo.get(\"language\")\n",
        "            }\n",
        "            if repo.get(\"language\") == \"Jupyter Notebook\":\n",
        "                repo_details[\"language\"] = \"Python\"\n",
        "\n",
        "            features[\"top_repositories\"].append(repo_details)\n",
        "            if repo.get(\"language\"):\n",
        "                features[\"languages\"].append(repo.get(\"language\"))\n",
        "            features[\"commit_count\"] += repo.get(\"stargazers_count\", 0)\n",
        "            if repo.get(\"pulls_url\"):\n",
        "                features[\"community_interaction\"] += 1\n",
        "\n",
        "        features[\"top_repositories\"] = sorted(\n",
        "            features[\"top_repositories\"], key=lambda x: x[\"stars\"], reverse=True\n",
        "        )[:5]\n",
        "\n",
        "        return features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {github_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def calculate_technical_score(features):\n",
        "    \"\"\"\n",
        "    Calculate technical expertise score based on GitHub profile features.\n",
        "\n",
        "    Args:\n",
        "        features (dict): Features extracted from GitHub profile\n",
        "\n",
        "    Returns:\n",
        "        float: Technical expertise score (0-100)\n",
        "    \"\"\"\n",
        "    if not features or not features.get(\"top_repositories\"):\n",
        "        return 0\n",
        "\n",
        "    code_quality_score = 0.80\n",
        "    tech_stack_score = min(1, len(set(features[\"languages\"])) / 10)\n",
        "    project_complexity_score = sum([repo[\"stars\"] for repo in features[\"top_repositories\"]]) / 500\n",
        "    project_complexity_score = min(project_complexity_score, 1)\n",
        "    contribution_score = features[\"community_interaction\"] / 10\n",
        "    testing_doc_score = 0.90\n",
        "\n",
        "    technical_score = (\n",
        "        (code_quality_score * 0.30) +\n",
        "        (tech_stack_score * 0.25) +\n",
        "        (project_complexity_score * 0.25) +\n",
        "        (contribution_score * 0.10) +\n",
        "        (testing_doc_score * 0.10)\n",
        "    )\n",
        "\n",
        "    return round(technical_score * 100, 2)\n",
        "\n",
        "def calculate_skills_score(skills_list):\n",
        "    \"\"\"\n",
        "    Calculate skills score based on the number of skills.\n",
        "\n",
        "    Args:\n",
        "        skills_list (list): List of skills\n",
        "\n",
        "    Returns:\n",
        "        float: Skills score (0-100)\n",
        "    \"\"\"\n",
        "    if not skills_list:\n",
        "        return 0.0\n",
        "    return min(len(skills_list) * 10, 100.0)\n",
        "\n",
        "def calculate_experience_score(years_of_experience):\n",
        "    \"\"\"\n",
        "    Calculate experience score based on years of experience.\n",
        "\n",
        "    Args:\n",
        "        years_of_experience (float): Years of professional experience\n",
        "\n",
        "    Returns:\n",
        "        float: Experience score (0-100)\n",
        "    \"\"\"\n",
        "    if not years_of_experience:\n",
        "        return 0.0\n",
        "    return min(years_of_experience * 10, 100.0)\n",
        "\n",
        "def calculate_dynamic_score(row, experience_weight=0.6, skills_weight=0.4):\n",
        "    \"\"\"\n",
        "    Calculate dynamic score for a profile based on skills and experience.\n",
        "    This function is used for profiles without GitHub links.\n",
        "\n",
        "    Args:\n",
        "        row (pandas.Series): The row of the DataFrame corresponding to the current profile.\n",
        "        experience_weight (float): Weight to assign to experience.\n",
        "        skills_weight (float): Weight to assign to skills.\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated dynamic score.\n",
        "    \"\"\"\n",
        "    # Calculate individual scores\n",
        "    skills_score = calculate_skills_score(row['skills'].split(','))  # assuming skills are comma-separated\n",
        "    experience_score = calculate_experience_score(row['experience_years'])\n",
        "\n",
        "    # Calculate weighted score\n",
        "    dynamic_score = (skills_score * skills_weight) + (experience_score * experience_weight)\n",
        "\n",
        "    return dynamic_score\n",
        "\n",
        "def process_github_links(csv_path, github_token):\n",
        "    \"\"\"\n",
        "    Process GitHub links in a CSV file and calculate technical expertise scores.\n",
        "\n",
        "    Args:\n",
        "        csv_path (str): Path to the input CSV file\n",
        "        github_token (str): GitHub API authentication token\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Updated DataFrame with technical expertise scores\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "        return None\n",
        "\n",
        "    github_column = 'github'\n",
        "\n",
        "    if github_column not in df.columns:\n",
        "        print(f\"Error: No column named '{github_column}' found in the CSV.\")\n",
        "        return None\n",
        "\n",
        "    df['technical_expertise_score'] = 0.0\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        github_url = row[github_column]\n",
        "        if pd.notna(github_url):\n",
        "            try:\n",
        "                features = extract_github_features(github_url, github_token)\n",
        "                if features:\n",
        "                    technical_score = calculate_technical_score(features)\n",
        "                    df.at[index, 'technical_expertise_score'] = technical_score\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {github_url}: {e}\")\n",
        "                df.at[index, 'technical_expertise_score'] = 0.0\n",
        "        else:\n",
        "            # If no GitHub URL is present, calculate dynamic score\n",
        "            dynamic_score = calculate_dynamic_score(row)\n",
        "            df.at[index, 'technical_expertise_score'] = dynamic_score\n",
        "\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the GitHub profile analysis.\n",
        "    \"\"\"\n",
        "    # Update these paths and token according to your environment\n",
        "    CSV_FILE_PATH = '/content/Book1.csv'\n",
        "    GITHUB_TOKEN = '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'\n",
        "    OUTPUT_FILE_PATH = '/content/Book1.csv'\n",
        "\n",
        "    updated_df = process_github_links(CSV_FILE_PATH, GITHUB_TOKEN)\n",
        "\n",
        "    if updated_df is not None:\n",
        "        updated_df.to_csv(OUTPUT_FILE_PATH, index=False)\n",
        "        print(f\"Analysis complete. Results saved to {OUTPUT_FILE_PATH}\")\n",
        "        print(\"\\nTop 5 Profiles by Technical Expertise Score:\")\n",
        "        top_profiles = updated_df.nlargest(5, 'technical_expertise_score')\n",
        "        print(top_profiles[['github', 'technical_expertise_score']])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0571QwWf3aAW",
        "outputId": "37840c17-9a56-40bb-9f66-b7dfa0898f1b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis complete. Results saved to /content/Book1.csv\n",
            "\n",
            "Top 5 Profiles by Technical Expertise Score:\n",
            "                               github  technical_expertise_score\n",
            "2                                 NaN                       66.0\n",
            "3                                 NaN                       60.0\n",
            "1       https://github.com/amiradridi                       54.5\n",
            "5                                 NaN                       44.0\n",
            "0  https://github.com/saikruthikreddy                       43.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LiccYlyO8Opi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ge2G3hu0DT-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}